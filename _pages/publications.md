---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

- Michael E. Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, Gabriel Peyré. *How do Transformers perform In-Context Autoregressive Learning?* [Preprint](https://arxiv.org/abs/2402.05787)

- Pierre Marion*, Yu-Han Wu*, Michael E. Sander, Gérard Biau. *Implicit regularization of deep residual networks towards neural ODEs*. ICLR, 2024 (Spotlight). [Paper](https://arxiv.org/abs/2309.01213)

- Michael E. Sander, Tom Sander, Maxime Sylvestre. *Unveiling the secrets of paintings: deep neural networks trained on high-resolution multispectral images for accurate attribution and authentication.* QCAV, 2023.

- Michael E. Sander, Joan Puigcerver, Josip Djolonga, Gabriel Peyré, Mathieu Blondel. *Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective.* ICML, 2023. [Paper](https://arxiv.org/abs/2302.01425)

- Michael E. Sander, Pierre Ablin, Gabriel Peyré. *Do Residual Neural Networks discretize Neural Ordinary Differential Equations?* NeurIPS, 2022. [Paper](https://arxiv.org/abs/2205.14612), [GitHub](https://github.com/michaelsdr/resnet_nodes)

- Samy Jelassi, Michael E. Sander, Yuanzhi Li. *Vision Transformers provably learn spatial structure.* NeurIPS, 2022. [Paper](https://arxiv.org/abs/2210.09221)

- Michael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyré. *Sinkformers: Transformers with Doubly Stochastic Attention.* AISTATS, 2022. [Paper](https://arxiv.org/abs/2110.11773), [GitHub](https://github.com/michaelsdr/sinkformers), [short presentation](https://slideslive.com/38980517/sinkformers-transformers-with-doubly-stochastic-attention?ref=speaker-17920)

- Michael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyré. *Momentum Residual Neural Networks.* ICML, 2021. [Paper](https://arxiv.org/abs/2102.07870), [GitHub](https://github.com/michaelsdr/momentumnet), [short presentation](https://www.youtube.com/watch?v=4PQR7ErASNo)
