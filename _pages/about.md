---
permalink: /
title: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Ph.D. student at [Ecole Normale Supérieure](https://www.ens.psl.eu/en) (ENS Paris). I work with [Gabriel Peyré](http://www.gpeyre.com/) and [Mathieu Blondel](https://mblondel.org/) on building and studying new deep learning models. 
I was a Student Researcher at [Google DeepMind](https://deepmind.google/) from September 2022 to March 2023. 

I graduated from [École Polytechnique](https://www.polytechnique.edu/en) in 2020 and have a master degree from [ENS Paris-Saclay](https://ens-paris-saclay.fr/en) in mathematics, vision and learning ([MVA](https://www.master-mva.com/)), as well as a master degree from Sorbonne Université in mathematics (modeling).

Research interests
---

My main research interests are at the intersection between deep learning, dynamical systems, optimal transport and differentiable learning. I am particularly interested in the neural ODE framework as a tool to design and study new deep architectures, among which ResNets and Transformers. I am also interested in the convergence of the hidden states trajectories of ResNets to the continuous one of Neural ODEs. 

Publications
---

- Michael E. Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, Gabriel Peyré. *How do Transformers perform In-Context Autoregressive Learning?*. ICML, 2024  [Paper](https://arxiv.org/abs/2402.05787), [GitHub](https://github.com/michaelsdr/ical) 


- Pierre Marion*, Yu-Han Wu*, Michael E. Sander, Gérard Biau. *Implicit regularization of deep residual networks towards neural ODEs*. ICLR, 2024 (Spotlight). [Paper](https://arxiv.org/abs/2309.01213), [GitHub](https://github.com/michaelsdr/implicit-regularization-resnets-nodes)

- Michael E. Sander, Tom Sander, Maxime Sylvestre. *Unveiling the secrets of paintings: deep neural networks trained on high-resolution multispectral images for accurate attribution and authentication.* QCAV, 2023.

- Michael E. Sander, Joan Puigcerver, Josip Djolonga, Gabriel Peyré, Mathieu Blondel. *Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective.* ICML, 2023. [Paper](https://arxiv.org/abs/2302.01425), [GitHub](https://github.com/google-research/google-research/tree/master/sparse_soft_topk)

- Michael E. Sander, Pierre Ablin, Gabriel Peyré. *Do Residual Neural Networks discretize Neural Ordinary Differential Equations?* NeurIPS, 2022. [Paper](https://arxiv.org/abs/2205.14612), [GitHub](https://github.com/michaelsdr/resnet_nodes)

- Samy Jelassi, Michael E. Sander, Yuanzhi Li. *Vision Transformers provably learn spatial structure.* NeurIPS, 2022. [Paper](https://arxiv.org/abs/2210.09221)

- Michael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyré. *Sinkformers: Transformers with Doubly Stochastic Attention.* AISTATS, 2022. [Paper](https://arxiv.org/abs/2110.11773), [GitHub](https://github.com/michaelsdr/sinkformers), [short presentation](https://slideslive.com/38980517/sinkformers-transformers-with-doubly-stochastic-attention?ref=speaker-17920)

- Michael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyré. *Momentum Residual Neural Networks.* ICML, 2021. [Paper](https://arxiv.org/abs/2102.07870), [GitHub](https://github.com/michaelsdr/momentumnet), [short presentation](https://www.youtube.com/watch?v=4PQR7ErASNo)

