---
permalink: /
title: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Research Scientist at [Google DeepMind](https://deepmind.google/).

Before that, I was a PhD student at [Ecole Normale Supérieure](https://www.ens.psl.eu/en) (ENS Paris), with [Gabriel Peyré](http://www.gpeyre.com/) and [Mathieu Blondel](https://mblondel.org/). 

I graduated from [École Polytechnique](https://www.polytechnique.edu/en) in 2020 and have a master degree from [ENS Paris-Saclay](https://ens-paris-saclay.fr/en) in mathematics, vision and learning ([MVA](https://www.master-mva.com/)), as well as a master degree from Sorbonne Université in mathematics (modeling).

Contact: michael.sander <at> polytechnique.org


Publications
---

- Michael E. Sander, Gabriel Peyré. *Towards Understanding the Universality of Transformers for Next-Token Prediction*. [Preprint](https://arxiv.org/abs/2410.03011).

- Michael E. Sander. *Deeper Learning: Residual Networks, Neural Differential Equations and Transformers, in Theory and Action*. [PhD Manuscript](https://michaelsdr.github.io/documents/Manuscript.pdf).

- Michael E. Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, Gabriel Peyré. *How do Transformers perform In-Context Autoregressive Learning?*. ICML, 2024  [Paper](https://arxiv.org/abs/2402.05787), [GitHub](https://github.com/michaelsdr/ical) 

- Pierre Marion*, Yu-Han Wu*, Michael E. Sander, Gérard Biau. *Implicit regularization of deep residual networks towards neural ODEs*. ICLR, 2024 (Spotlight). [Paper](https://arxiv.org/abs/2309.01213), [GitHub](https://github.com/michaelsdr/implicit-regularization-resnets-nodes)

- Michael E. Sander, Joan Puigcerver, Josip Djolonga, Gabriel Peyré, Mathieu Blondel. *Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective.* ICML, 2023. [Paper](https://arxiv.org/abs/2302.01425), [GitHub](https://github.com/google-research/google-research/tree/master/sparse_soft_topk)

- Michael E. Sander, Pierre Ablin, Gabriel Peyré. *Do Residual Neural Networks discretize Neural Ordinary Differential Equations?* NeurIPS, 2022. [Paper](https://arxiv.org/abs/2205.14612), [GitHub](https://github.com/michaelsdr/resnet_nodes)

- Samy Jelassi, Michael E. Sander, Yuanzhi Li. *Vision Transformers provably learn spatial structure.* NeurIPS, 2022. [Paper](https://arxiv.org/abs/2210.09221)

- Michael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyré. *Sinkformers: Transformers with Doubly Stochastic Attention.* AISTATS, 2022. [Paper](https://arxiv.org/abs/2110.11773), [GitHub](https://github.com/michaelsdr/sinkformers), [short presentation](https://slideslive.com/38980517/sinkformers-transformers-with-doubly-stochastic-attention?ref=speaker-17920)

- Michael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyré. *Momentum Residual Neural Networks.* ICML, 2021. [Paper](https://arxiv.org/abs/2102.07870), [GitHub](https://github.com/michaelsdr/momentumnet), [short presentation](https://www.youtube.com/watch?v=4PQR7ErASNo)

