---
permalink: /
title: "--"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Research Scientist at [Google DeepMind](https://deepmind.google/).

Before that, I obtained my PhD degree at [Ecole Normale Supérieure](https://www.ens.psl.eu/en) (ENS Paris), under the supervision of [Gabriel Peyré](http://www.gpeyre.com/) and [Mathieu Blondel](https://mblondel.org/). 

I graduated from [École Polytechnique](https://www.polytechnique.edu/en) (X2016) and have a master degree from [ENS Paris-Saclay](https://ens-paris-saclay.fr/en) in mathematics, vision and learning ([MVA](https://www.master-mva.com/)), as well as a master degree from Sorbonne Université in mathematics ([Modelling](https://www.ip-paris.fr/education/masters/mention-mathematiques-appliquees-statistique/master-year-2-mathematical-modelling)).

Contact: michael (dot) sander (at) polytechnique (dot) org


Publications
---

- Lev Fedorov, Michaël E. Sander, Romuald Elie, Pierre Marion, Mathieu Laurière. *Clustering in Deep Stochastic Transformers.* Preprint, 2026. [Paper](https://arxiv.org/abs/2601.21942)￼

- Germain Vivier-Ardisson, Michaël E. Sander, Axel Parmentier, Mathieu Blondel. *Differentiable Knapsack and Top-k Operators via Dynamic Programming.* Preprint, 2026. [Paper](https://arxiv.org/abs/2601.21775)

- Mathieu Blondel, Michaël E. Sander, Germain Vivier-Ardisson, Tianlin Liu, Vincent Roulet. *Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction.* Preprint, 2025. [Paper](https://arxiv.org/abs/2512.15605)

- Michaël E. Sander, Vincent Roulet, Tianlin Liu, Mathieu Blondel. *Joint Learning of Energy-based Models and their Partition Function.* ICML, 2025. [Paper](https://arxiv.org/abs/2501.18528)

- Vincent Roulet, Tianlin Liu, Nino Vieillard, Michael E. Sander, Mathieu Blondel. *Loss Functions and Operators Generated by f-Divergences.* ICML, 2025. [Paper](https://arxiv.org/abs/2501.18537)

- Michaël E. Sander, Gabriel Peyré. *Towards Understanding the Universality of Transformers for Next-Token Prediction*. ICLR, 2025. [Paper](https://arxiv.org/abs/2410.03011).

- Michaël E. Sander. *Deeper Learning: Residual Networks, Neural Differential Equations and Transformers, in Theory and Action*. [PhD Manuscript](https://michaelsdr.github.io/documents/Manuscript.pdf).

- Michaël E. Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, Gabriel Peyré. *How do Transformers perform In-Context Autoregressive Learning?*. ICML, 2024.  [Paper](https://arxiv.org/abs/2402.05787), [GitHub](https://github.com/michaelsdr/ical) 

- Pierre Marion*, Yu-Han Wu*, Michaël E. Sander, Gérard Biau. *Implicit regularization of deep residual networks towards neural ODEs*. ICLR, 2024 (Spotlight). [Paper](https://arxiv.org/abs/2309.01213), [GitHub](https://github.com/michaelsdr/implicit-regularization-resnets-nodes)

- Michaël E. Sander, Joan Puigcerver, Josip Djolonga, Gabriel Peyré, Mathieu Blondel. *Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective.* ICML, 2023. [Paper](https://arxiv.org/abs/2302.01425), [GitHub](https://github.com/google-research/google-research/tree/master/sparse_soft_topk)

- Michaël E. Sander, Pierre Ablin, Gabriel Peyré. *Do Residual Neural Networks discretize Neural Ordinary Differential Equations?* NeurIPS, 2022. [Paper](https://arxiv.org/abs/2205.14612), [GitHub](https://github.com/michaelsdr/resnet_nodes)

- Samy Jelassi, Michaël E. Sander, Yuanzhi Li. *Vision Transformers provably learn spatial structure.* NeurIPS, 2022. [Paper](https://arxiv.org/abs/2210.09221)

- Michaël E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyré. *Sinkformers: Transformers with Doubly Stochastic Attention.* AISTATS, 2022. [Paper](https://arxiv.org/abs/2110.11773), [GitHub](https://github.com/michaelsdr/sinkformers), [short presentation](https://slideslive.com/38980517/sinkformers-transformers-with-doubly-stochastic-attention?ref=speaker-17920)

- Michaël E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyré. *Momentum Residual Neural Networks.* ICML, 2021. [Paper](https://arxiv.org/abs/2102.07870), [GitHub](https://github.com/michaelsdr/momentumnet), [short presentation](https://www.youtube.com/watch?v=4PQR7ErASNo)

